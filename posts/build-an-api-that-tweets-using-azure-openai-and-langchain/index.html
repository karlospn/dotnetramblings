<!DOCTYPE html>
<html lang="en">
  <head>
    
      <title>Building a serverless API that tweets about my blog posts using Azure OpenAI and LangChain :: my tech ramblings — A blog for writing about my techie ramblings</title>
    
    <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
<meta name="description" content="Just show me the code!
As always, if you don’t care about the post I have uploaded the source code on my Github.
The other day I thought that maybe I should start promoting in Twitter my blog posts, a good starting point would be to create a new tweet with a brief summary every time a new post gets published in my blog.
The truth is that I&amp;rsquo;m not a big fan of Twitter and I don&amp;rsquo;t want to have to manually write tweets, so I&amp;rsquo;ve thought about automating the creation of these tweets in some way."/>
<meta name="keywords" content=""/>
<meta name="robots" content="noodp"/>
<link rel="canonical" href="https://www.mytechramblings.com/posts/build-an-api-that-tweets-using-azure-openai-and-langchain/" />





<link rel="stylesheet" href="https://www.mytechramblings.com/assets/style.css">


<link rel="stylesheet" href="https://www.mytechramblings.com/style.css">


<link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://www.mytechramblings.com/img/apple-touch-icon-144-precomposed.png">
<link rel="shortcut icon" href="https://www.mytechramblings.com/img/favicon.png">


<meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Building a serverless API that tweets about my blog posts using Azure OpenAI and LangChain"/>
<meta name="twitter:description" content="This post is going to show you a simple way of building a serverless Python API that uses LangChain and Azure OpenAI to create tweets summarizing the content of my blog posts."/>



<meta property="og:title" content="Building a serverless API that tweets about my blog posts using Azure OpenAI and LangChain" />
<meta property="og:description" content="This post is going to show you a simple way of building a serverless Python API that uses LangChain and Azure OpenAI to create tweets summarizing the content of my blog posts." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.mytechramblings.com/posts/build-an-api-that-tweets-using-azure-openai-and-langchain/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-04-04T10:01:11+02:00" />
<meta property="article:modified_time" content="2023-04-04T10:01:11+02:00" />







  </head>
  <body class="dark-theme">
    <div class="container">
      <header class="header">
  <span class="header__inner">
    <a href="https://www.mytechramblings.com/" class="logo" style="text-decoration: none;">
  
    <span class="logo__mark"><svg xmlns="http://www.w3.org/2000/svg" class="greater-icon" viewBox="0 0 44 44">
  <path fill="none" d="M15 8l14.729 14.382L15 35.367"/>
</svg>
</span>
    <span class="logo__text">my tech ramblings</span>
    <span class="logo__cursor"></span>
  
</a>

    <span class="header__right">
      
        <nav class="menu">
  <ul class="menu__inner menu__inner--desktop">
    
      
        
          <li><a href="/about">About</a></li>
        
      
      
    
  </ul>

  <ul class="menu__inner menu__inner--mobile">
    
      
        <li><a href="/about">About</a></li>
      
    
  </ul>
</nav>

        <span class="menu-trigger">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M0 0h24v24H0z" fill="none"/>
            <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/>
          </svg>
        </span>
      
      <span class="theme-toggle">
        <svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg">
  <path d="M22 41C32.4934 41 41 32.4934 41 22C41 11.5066 32.4934 3 22
  3C11.5066 3 3 11.5066 3 22C3 32.4934 11.5066 41 22 41ZM7 22C7
  13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22Z"/>
</svg>

      </span>
    </span>
  </span>
</header>


      <div class="content">
        
  
  

  <div class="post">
    <h1 class="post-title"><a href="https://www.mytechramblings.com/posts/build-an-api-that-tweets-using-azure-openai-and-langchain/">Building a serverless API that tweets about my blog posts using Azure OpenAI and LangChain</a></h1>
    <div class="post-meta">
      
        <span class="post-date">
          2023-04-04
        </span>

        
          
            



          
        
      

      
      
        <span class="post-read-time">— 11 min read</span>
      
    </div>

    
      <span class="post-tags">
        
          #<a href="https://www.mytechramblings.com/tags/azure/">azure</a>&nbsp;
        
          #<a href="https://www.mytechramblings.com/tags/openai/">openai</a>&nbsp;
        
          #<a href="https://www.mytechramblings.com/tags/python/">python</a>&nbsp;
        
          #<a href="https://www.mytechramblings.com/tags/langchain/">langchain</a>&nbsp;
        
          #<a href="https://www.mytechramblings.com/tags/ai/">ai</a>&nbsp;
        
          #<a href="https://www.mytechramblings.com/tags/serverless/">serverless</a>&nbsp;
        
      </span>
    

    

    <div class="post-content">
      
      <blockquote>
<p><strong>Just show me the code!</strong><br>
As always, if you don’t care about the post I have uploaded the source code on my <a href="https://github.com/karlospn/building-a-tweeting-api-using-openai-and-azure-functions">Github</a>.</p>
</blockquote>
<p>The other day I thought that maybe I should start promoting in Twitter my blog posts, a good starting point would be to create a new tweet with a brief summary every time a new post gets published in my blog.</p>
<p>The truth is that I&rsquo;m not a big fan of Twitter and I don&rsquo;t want to have to manually write tweets, so I&rsquo;ve thought about automating the creation of these tweets in some way. <br>
The hottest topic right now is undoubtedly ChatGPT and  Large Language Models (LLM), so I thought, &ldquo;What if I let ChatGPT write these tweets recommending my posts for me?&rdquo;.</p>
<p>In the end, I&rsquo;ve come to the conclusion that I&rsquo;m going to build an API that takes one of my post&rsquo;s URLs as a parameter and is responsible for:</p>
<ul>
<li>Retrieving the text from my post.</li>
<li>Calling ChatGPT to draft a tweet summarizing the content of my post.</li>
<li>Posting it on Twitter.</li>
</ul>
<p><strong>In this post, I&rsquo;m going to focus on setting up this API, so if you&rsquo;re interested, keep reading.</strong></p>
<h1 id="application-to-build"><strong>Application to build</strong></h1>
<p>This API is only going to be used when a new post is published, so it doesn&rsquo;t make sense to continuously host it on services like App Services. For this use case, the best option is to go serverless and use a service like <strong>Azure Functions</strong>.</p>
<p>Instead of using ChatGPT, I&rsquo;m going to use its Microsoft counterpart: <strong>Azure OpenAI</strong>. Right now, this service is in preview, so in order to use it, you need to fill out a form and wait for Microsoft to grant you access.</p>
<p>I&rsquo;m going to develop the API using <strong>Python</strong> and to communicate with OpenAI, I&rsquo;ll be using the <strong><a href="https://python.langchain.com/en/latest/index.html">LangChain</a></strong> library.</p>
<h1 id="what-is-langchain"><strong>What is LangChain?</strong></h1>
<p>At its core, LangChain is a framework built around LLMs. It provides a simple and intuitive interface to interact with LLM models.</p>
<p>LangChain provides a set of abstractions that handle all the necessary steps for interacting with the LLM model, including authentication, input preprocessing, and result post-processing. With LangChain, you can quickly generate natural language text that is tailored to your specific needs.</p>
<p>LangChain also provides a set of implementations for the most well-known LLM interaction strategies.</p>
<p>The core idea of the library is that we can chain together different components to create more advanced use cases around LLMs. Chains may consist of multiple components from several modules:</p>
<ul>
<li>Prompt templates: Prompt templates are templates for different types of prompts.</li>
<li>LLMs: Large language models like GPT-3, BLOOM, etc</li>
<li>Agents: Agents use LLMs to decide what actions should be taken.</li>
<li>Memory: Short-term memory, long-term memory.</li>
</ul>
<p>If you want to know more about it, visit its site:</p>
<ul>
<li><a href="https://python.langchain.com/en/latest/index.html">https://python.langchain.com/en/latest/index.html</a></li>
</ul>
<h1 id="how-the-app-works"><strong>How the app works</strong></h1>
<p>The following diagram shows how the API is going to work:</p>
<p><img alt="tweetapi-process-diagram" src="/img/tweetapi-process-diagram.png"></p>
<p>The principle is really simple:</p>
<ol>
<li>The API must be called with a parameter called <code>uri</code> in the Http request body, here&rsquo;s an example:</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>curl -X POST https://func-openai-azfunc-dev.azurewebsites.net/api/tweet?code<span style="color:#f92672">=</span>hoZ6u8lIMlhu7-Zs8gvDb04R2fXvYeapijR3YYRlgiwmAzFulsiRMA<span style="color:#f92672">==</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>     -H <span style="color:#e6db74">&#34;Content-Type: application/json&#34;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>     -d <span style="color:#e6db74">&#39;{&#34;uri&#34;: &#34;https://www.mytechramblings.com/posts/deploy-az-resources-when-not-available-on-azurerm/&#34;}&#39;</span>
</span></span></code></pre></div><p>The parameter <code>uri</code> indicates the website from where to obtain the content that will get summarized.</p>
<ol start="2">
<li>The API fetches the content from the <code>uri</code> website.</li>
<li>The text content is split into multiple chunks. The reason to split it into multiple parts is that the text might be too long to be summarized by a single request to Azure OpenAI.</li>
<li>Every chunk gets summarized independently using Azure OpenAI.</li>
<li>All the summarized chunks are put together in a single piece of text and summarized again with Azure OpenAI. The resulting text is the tweet.</li>
<li>The tweet gets posted to Twitter.</li>
</ol>
<h1 id="building-the-tweeting-api"><strong>Building the tweeting API</strong></h1>
<p>Let me show you the final result, and from there, I&rsquo;ll explain to you line by line what the API is doing.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> azure.functions <span style="color:#66d9ef">as</span> func
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> os
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> tweepy
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.text_splitter <span style="color:#f92672">import</span> RecursiveCharacterTextSplitter
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.document_loaders <span style="color:#f92672">import</span> UnstructuredURLLoader
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.chains.summarize <span style="color:#f92672">import</span> load_summarize_chain
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.chat_models <span style="color:#f92672">import</span> AzureChatOpenAI
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.prompts <span style="color:#f92672">import</span> PromptTemplate
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>prompt_template <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">You are the author of the source text.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">You need to write a tweet that summarizes the source text.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">The tweet must not contain any kind of code.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">Make sure the tweet is compelling and well-written. 
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">The tweet must end with the following phrase: &#39;More details in: www.mytechramblings.com&#39;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">The total tweet size must have no more than 270 characters.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">SOURCE:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74"></span><span style="color:#e6db74">{text}</span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">TWEET IN ENGLISH:&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>app <span style="color:#f92672">=</span> func<span style="color:#f92672">.</span>FunctionApp()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">@app.function_name</span>(name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;TweetApi&#34;</span>)
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">@app.route</span>(route<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;tweet&#34;</span>)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">test_function</span>(req: func<span style="color:#f92672">.</span>HttpRequest) <span style="color:#f92672">-&gt;</span> func<span style="color:#f92672">.</span>HttpResponse:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">try</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Get twitter credentials</span>
</span></span><span style="display:flex;"><span>        twitter_consumer_key <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>environ[<span style="color:#e6db74">&#39;TWITTER_CONSUMER_KEY&#39;</span>]
</span></span><span style="display:flex;"><span>        twitter_consumer_secret <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>environ[<span style="color:#e6db74">&#39;TWITTER_CONSUMER_SECRET&#39;</span>]
</span></span><span style="display:flex;"><span>        twitter_access_token <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>environ[<span style="color:#e6db74">&#39;TWITTER_ACCESS_TOKEN&#39;</span>]
</span></span><span style="display:flex;"><span>        twitter_access_token_secret <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>environ[<span style="color:#e6db74">&#39;TWITTER_ACCESS_TOKEN_SECRET&#39;</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Get Azure OpenAi credentials</span>
</span></span><span style="display:flex;"><span>        az_open_ai_url <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>environ[<span style="color:#e6db74">&#39;OPENAI_URL&#39;</span>]
</span></span><span style="display:flex;"><span>        az_open_ai_apikey <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>environ[<span style="color:#e6db74">&#39;OPENAI_APIKEY&#39;</span>]
</span></span><span style="display:flex;"><span>        az_open_ai_deployment_name <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>environ[<span style="color:#e6db74">&#39;OPENAI_DEPLOYMENT_NAME&#39;</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">except</span> <span style="color:#a6e22e">KeyError</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> func<span style="color:#f92672">.</span>HttpResponse(<span style="color:#e6db74">&#34;Something went wrong when trying to retrieve the credentials&#34;</span>, status_code<span style="color:#f92672">=</span><span style="color:#ae81ff">500</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">try</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Read uri from HTTP request</span>
</span></span><span style="display:flex;"><span>        req_body <span style="color:#f92672">=</span> req<span style="color:#f92672">.</span>get_json()
</span></span><span style="display:flex;"><span>        uri <span style="color:#f92672">=</span> req_body<span style="color:#f92672">.</span>get(<span style="color:#e6db74">&#39;uri&#39;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">except</span> <span style="color:#a6e22e">ValueError</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> func<span style="color:#f92672">.</span>HttpResponse( <span style="color:#e6db74">&#34;The request has a missing body.&#34;</span>, status_code<span style="color:#f92672">=</span><span style="color:#ae81ff">400</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> uri:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> func<span style="color:#f92672">.</span>HttpResponse( <span style="color:#e6db74">&#34;The &#39;uri&#39; attribute is missing in the request body.&#34;</span>,  status_code<span style="color:#f92672">=</span><span style="color:#ae81ff">400</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">try</span>:
</span></span><span style="display:flex;"><span>            
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># Create Azure OpenAI client</span>
</span></span><span style="display:flex;"><span>            llm <span style="color:#f92672">=</span> AzureChatOpenAI(
</span></span><span style="display:flex;"><span>                openai_api_base<span style="color:#f92672">=</span>az_open_ai_url,
</span></span><span style="display:flex;"><span>                openai_api_version<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;2023-03-15-preview&#34;</span>,
</span></span><span style="display:flex;"><span>                deployment_name<span style="color:#f92672">=</span>az_open_ai_deployment_name,
</span></span><span style="display:flex;"><span>                openai_api_key<span style="color:#f92672">=</span>az_open_ai_apikey,
</span></span><span style="display:flex;"><span>                openai_api_type <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;azure&#34;</span>,
</span></span><span style="display:flex;"><span>            ) 
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># Read context from URI</span>
</span></span><span style="display:flex;"><span>            loader <span style="color:#f92672">=</span> UnstructuredURLLoader(urls<span style="color:#f92672">=</span>[uri])
</span></span><span style="display:flex;"><span>            data <span style="color:#f92672">=</span> loader<span style="color:#f92672">.</span>load()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># Split content into chunks</span>
</span></span><span style="display:flex;"><span>            text_splitter <span style="color:#f92672">=</span> RecursiveCharacterTextSplitter(chunk_size<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>, chunk_overlap<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>)
</span></span><span style="display:flex;"><span>            texts <span style="color:#f92672">=</span> text_splitter<span style="color:#f92672">.</span>split_documents(data)
</span></span><span style="display:flex;"><span>            
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># Run langchain &#39;map_reduce&#39; chain</span>
</span></span><span style="display:flex;"><span>            prompt <span style="color:#f92672">=</span> PromptTemplate(template<span style="color:#f92672">=</span>prompt_template, input_variables<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#34;text&#34;</span>])
</span></span><span style="display:flex;"><span>            chain <span style="color:#f92672">=</span> load_summarize_chain(llm, chain_type<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;map_reduce&#34;</span>, combine_prompt<span style="color:#f92672">=</span>prompt)
</span></span><span style="display:flex;"><span>            output <span style="color:#f92672">=</span> chain<span style="color:#f92672">.</span>run(texts)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># Print tweet content</span>
</span></span><span style="display:flex;"><span>            print(output)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># Create Twitter client</span>
</span></span><span style="display:flex;"><span>            client <span style="color:#f92672">=</span> tweepy<span style="color:#f92672">.</span>Client(
</span></span><span style="display:flex;"><span>                consumer_key<span style="color:#f92672">=</span>twitter_consumer_key, consumer_secret<span style="color:#f92672">=</span>twitter_consumer_secret,
</span></span><span style="display:flex;"><span>                access_token<span style="color:#f92672">=</span>twitter_access_token, access_token_secret<span style="color:#f92672">=</span>twitter_access_token_secret
</span></span><span style="display:flex;"><span>            )
</span></span><span style="display:flex;"><span>      
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># Create tweet using langchain &#39;map_reduce&#39; chain output</span>
</span></span><span style="display:flex;"><span>            response <span style="color:#f92672">=</span> client<span style="color:#f92672">.</span>create_tweet(
</span></span><span style="display:flex;"><span>                text<span style="color:#f92672">=</span>output
</span></span><span style="display:flex;"><span>            )
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">return</span> func<span style="color:#f92672">.</span>HttpResponse(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;https://twitter.com/user/status/</span><span style="color:#e6db74">{</span>response<span style="color:#f92672">.</span>data[<span style="color:#e6db74">&#39;id&#39;</span>]<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">except</span> tweepy<span style="color:#f92672">.</span>TweepyException <span style="color:#66d9ef">as</span> e:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">return</span> func<span style="color:#f92672">.</span>HttpResponse(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Something went wrong when creating the tweet: </span><span style="color:#e6db74">{</span>e<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>, status_code<span style="color:#f92672">=</span><span style="color:#ae81ff">500</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">except</span> <span style="color:#a6e22e">Exception</span> <span style="color:#66d9ef">as</span> ex:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">return</span> func<span style="color:#f92672">.</span>HttpResponse(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Something went wrong: </span><span style="color:#e6db74">{</span>ex<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>, status_code<span style="color:#f92672">=</span><span style="color:#ae81ff">500</span>)
</span></span></code></pre></div><p>As you can see, the implementation is no more than 50 lines of Python, which is largely thanks to the LangChain library, as it implements some of the most well-known patterns for interacting with an LLM model, this allows us to implement the <code>map-reduce</code> strategy with just a couples lines of code.</p>
<p>But before we start talking about the <code>map-reduce</code> strategy, <strong>let&rsquo;s review each line of the API and explain what I&rsquo;m doing.</strong></p>
<h2 id="1-set-azure-openai--twitter-credentials"><strong>1. Set Azure OpenAI &amp; Twitter credentials</strong></h2>
<p>A series of credentials are required to communicate with Azure OpenAI and the Twitter API. In the following code block, we are simply obtaining these credentials from configuration.</p>
<ul>
<li><code>TWITTER_CONSUMER_KEY</code>: Twitter API consumer key.</li>
<li><code>TWITTER_CONSUMER_SECRET</code>: Twitter API consumer secret.</li>
<li><code>TWITTER_ACCESS_TOKEN</code>: Twitter API access token.</li>
<li><code>TWITTER_ACCESS_TOKEN_SECRET</code>: Twitter API access token secret.</li>
<li><code>OPENAI_URL</code>: The URL of your Azure OpenAI service.
<ul>
<li>It has the following format: <code>https://{base}.openai.azure.com</code></li>
</ul>
</li>
<li><code>OPENAI_DEPLOYMENT_NAME</code>: The Azure OpenAI model deployment name.</li>
<li><code>OPENAI_APIKEY</code>: The Azure OpenAI api key.</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>    <span style="color:#66d9ef">try</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Get twitter credentials</span>
</span></span><span style="display:flex;"><span>        twitter_consumer_key <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>environ[<span style="color:#e6db74">&#39;TWITTER_CONSUMER_KEY&#39;</span>]
</span></span><span style="display:flex;"><span>        twitter_consumer_secret <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>environ[<span style="color:#e6db74">&#39;TWITTER_CONSUMER_SECRET&#39;</span>]
</span></span><span style="display:flex;"><span>        twitter_access_token <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>environ[<span style="color:#e6db74">&#39;TWITTER_ACCESS_TOKEN&#39;</span>]
</span></span><span style="display:flex;"><span>        twitter_access_token_secret <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>environ[<span style="color:#e6db74">&#39;TWITTER_ACCESS_TOKEN_SECRET&#39;</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Get Azure OpenAi credentials</span>
</span></span><span style="display:flex;"><span>        az_open_ai_url <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>environ[<span style="color:#e6db74">&#39;OPENAI_URL&#39;</span>]
</span></span><span style="display:flex;"><span>        az_open_ai_apikey <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>environ[<span style="color:#e6db74">&#39;OPENAI_APIKEY&#39;</span>]
</span></span><span style="display:flex;"><span>        az_open_ai_deployment_name <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>environ[<span style="color:#e6db74">&#39;OPENAI_DEPLOYMENT_NAME&#39;</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">except</span> <span style="color:#a6e22e">KeyError</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> func<span style="color:#f92672">.</span>HttpResponse(<span style="color:#e6db74">&#34;Something went wrong when trying to retrieve the credentials&#34;</span>, status_code<span style="color:#f92672">=</span><span style="color:#ae81ff">500</span>)
</span></span></code></pre></div><h2 id="2-get-the-uri-property-from-the-http-body-request"><strong>2. Get the &lsquo;uri&rsquo; property from the HTTP body request</strong></h2>
<p>The API will be called with a parameter called <code>uri</code> in the Http request body, here&rsquo;s an example:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>curl -X POST https://func-openai-azfunc-dev.azurewebsites.net/api/tweet?code<span style="color:#f92672">=</span>hoZ6u8lIMlhu7-Zs8gvDb04R2fXvYeapijR3YYRlgiwmAzFulsiRMA<span style="color:#f92672">==</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>     -H <span style="color:#e6db74">&#34;Content-Type: application/json&#34;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>     -d <span style="color:#e6db74">&#39;{&#34;uri&#34;: &#34;https://www.mytechramblings.com/posts/deploy-az-resources-when-not-available-on-azurerm/&#34;}&#39;</span>
</span></span></code></pre></div><p>This <code>uri</code> parameter will indicate the website URL from where to obtain the content that will get summarized.</p>
<p>In the following code block, we are obtaining the <code>uri</code> property from the Http request body. If it doesn&rsquo;t exist, the API responds with a 400 status code.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>    <span style="color:#66d9ef">try</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Read uri from HTTP request</span>
</span></span><span style="display:flex;"><span>        req_body <span style="color:#f92672">=</span> req<span style="color:#f92672">.</span>get_json()
</span></span><span style="display:flex;"><span>        uri <span style="color:#f92672">=</span> req_body<span style="color:#f92672">.</span>get(<span style="color:#e6db74">&#39;uri&#39;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">except</span> <span style="color:#a6e22e">ValueError</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> func<span style="color:#f92672">.</span>HttpResponse( <span style="color:#e6db74">&#34;The request has a missing body.&#34;</span>, status_code<span style="color:#f92672">=</span><span style="color:#ae81ff">400</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> uri:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> func<span style="color:#f92672">.</span>HttpResponse( <span style="color:#e6db74">&#34;The &#39;uri&#39; attribute is missing in the request body.&#34;</span>,  status_code<span style="color:#f92672">=</span><span style="color:#ae81ff">400</span>)
</span></span></code></pre></div><h2 id="3-read-and-split-the-blog-post-content"><strong>3. Read and split the blog post content</strong></h2>
<p>Once we know the URL from which we need to read the content, we use the Langchain <code>UnstructuredURLLoader</code> functionality to obtain it.<br>
The <code>UnstructuredURLLoader</code> uses the <code>Unstructured</code> python package under the hood. This package is a great way to transform all types of files - text, powerpoint, images, html, pdf, etc - into text data.</p>
<p>Apart from reading the content, in the following code block we&rsquo;re setting up the <code>AzureChatOpenAi</code> class, this class will be used in the next section.</p>
<p>The <code>AzureChatOpenAi</code> class allows us to communicate with the desired LLM model from those available in Azure OpenAI. In our case, we are using a <code>gpt-3.5-turbo</code> model, since as of today, I still do not have access to the <code>gpt-4</code> model.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>    <span style="color:#75715e"># Create Azure OpenAI client</span>
</span></span><span style="display:flex;"><span>    llm <span style="color:#f92672">=</span> AzureChatOpenAI(
</span></span><span style="display:flex;"><span>        openai_api_base<span style="color:#f92672">=</span>az_open_ai_url,
</span></span><span style="display:flex;"><span>        openai_api_version<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;2023-03-15-preview&#34;</span>,
</span></span><span style="display:flex;"><span>        deployment_name<span style="color:#f92672">=</span>az_open_ai_deployment_name,
</span></span><span style="display:flex;"><span>        openai_api_key<span style="color:#f92672">=</span>az_open_ai_apikey,
</span></span><span style="display:flex;"><span>        openai_api_type <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;azure&#34;</span>,
</span></span><span style="display:flex;"><span>    ) 
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Read context from URI</span>
</span></span><span style="display:flex;"><span>    loader <span style="color:#f92672">=</span> UnstructuredURLLoader(urls<span style="color:#f92672">=</span>[uri])
</span></span><span style="display:flex;"><span>    data <span style="color:#f92672">=</span> loader<span style="color:#f92672">.</span>load()
</span></span></code></pre></div><h2 id="4-create-the-tweet-content-using-a-map-reduce-strategy-and-a-custom-prompt-template"><strong>4. Create the tweet content using a &lsquo;map-reduce&rsquo; strategy and a custom prompt template</strong></h2>
<p>LLMs have a limited token length, which means we cannot pass it an entire wall of text at once to summarize, we need a more clever technique to decompose it, this is where the <code>map-reduce</code> strategy comes into play.</p>
<p>The <code>map-reduce</code> strategy is one of the multiple strategies available today for interacting with LLM models. It consists in:</p>
<ul>
<li>Splitting a text into multiples chunks of data.
<ul>
<li>To split the text into multiple chunks, we will use the Langchain <code>RecursiveCharacterTextSplitter</code> functionality.</li>
</ul>
</li>
<li>Pass every chunk of data through the language model to generate multiple responses.</li>
<li>Combine the multiple responses into a single one and then pass it again through the language model to obtain a definitive response.</li>
</ul>
<p>As you can imagine, this technique requires more than one call to the LLM/Azure OpenAI.</p>
<p>The next diagram shows the <code>map-reduce</code> strategy that we&rsquo;re going to implement:</p>
<p><img alt="tweetapi-map-reduce-diagram" src="/img/tweetapi-map-reduce-diagram.png"></p>
<p>The <code>map-reduce</code> strategy might seem quite complex to implement, but LangChain provides a set of implementations for the most well-known LLM interaction strategies, which means that implementing the <code>map-reduce</code> strategy with LangChain becomes a simple one-liner:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>    <span style="color:#f92672">from</span> langchain.chains.summarize <span style="color:#f92672">import</span> load_summarize_chain
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">...</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    load_summarize_chain(llm, chain_type<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;map_reduce&#34;</span>)
</span></span></code></pre></div><p>In the following code block, we&rsquo;re spliting the text into multiple chunks using the Langchain <code>RecursiveCharacterTextSplitter</code> functionality, and then we apply the <code>map-reduce</code> strategy using the <code>load_summarize_chain</code> function. The end result is the tweet that is going to be posted to Twitter.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>    <span style="color:#75715e"># Split content into chunks</span>
</span></span><span style="display:flex;"><span>    text_splitter <span style="color:#f92672">=</span> RecursiveCharacterTextSplitter(chunk_size<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>, chunk_overlap<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>)
</span></span><span style="display:flex;"><span>    texts <span style="color:#f92672">=</span> text_splitter<span style="color:#f92672">.</span>split_documents(data)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Run langchain &#39;map_reduce&#39; chain</span>
</span></span><span style="display:flex;"><span>    prompt <span style="color:#f92672">=</span> PromptTemplate(template<span style="color:#f92672">=</span>prompt_template, input_variables<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#34;text&#34;</span>])
</span></span><span style="display:flex;"><span>    chain <span style="color:#f92672">=</span> load_summarize_chain(llm, chain_type<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;map_reduce&#34;</span>, combine_prompt<span style="color:#f92672">=</span>prompt)
</span></span><span style="display:flex;"><span>    output <span style="color:#f92672">=</span> chain<span style="color:#f92672">.</span>run(texts)
</span></span></code></pre></div><p>Langchain applies a default prompt every time it sends text to the LLM model. In our case, we want to modify the combine prompt that is sent to the LLM model to fit Twitter&rsquo;s constraints.</p>
<p>We can use the <code>combine_prompt</code> parameter from the <code>load_summarize_chain</code> function to set the prompt that is going to be sent when trying to summarize the compacted chunks.</p>
<p>We can also customize the prompts used to generate the summary of every single chunk using the <code>map_prompt</code> parameter, but I don&rsquo;t need it, the default summarization prompt is good enough.</p>
<p><img alt="tweetapi-map-reduce-with-prompts-diagram" src="/img/tweetapi-map-reduce-with-prompts-diagram.png"></p>
<p>After some prompt engineering and some trial and error, here&rsquo;s how the combine prompt ended up looking like:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>    prompt_template <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    You are the author of the source text.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    You need to write a tweet that summarizes the source text.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    The tweet must not contain any kind of code.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Make sure the tweet is compelling and well-written. 
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    The tweet must end with the following phrase: &#39;More details in: www.mytechramblings.com&#39;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    The total tweet size must have no more than 270 characters.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    SOURCE:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    </span><span style="color:#e6db74">{text}</span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    TWEET IN ENGLISH:&#34;&#34;&#34;</span>
</span></span></code></pre></div><p>LangChain will combine the multiple summarized chunks into a single text and replace the <em>{text}</em> placeholder with it, and afterwards it will sent the resulting prompt to Azure OpenAI to generate the tweet.</p>
<h2 id="5-post-the-new-tweet-using-the-twitter-api"><strong>5. Post the new tweet using the Twitter Api</strong></h2>
<p>Once we have the tweet we are going to create, we can use the Python library <a href="https://www.tweepy.org/">tweepy</a> to create it.</p>
<p>In the following code block, we are posting the new tweet to Twitter using tweepy and returning the tweet URI to the client. As you can see, the code is descriptive enough.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>    <span style="color:#75715e"># Create Twitter client</span>
</span></span><span style="display:flex;"><span>    client <span style="color:#f92672">=</span> tweepy<span style="color:#f92672">.</span>Client(
</span></span><span style="display:flex;"><span>        consumer_key<span style="color:#f92672">=</span>twitter_consumer_key, consumer_secret<span style="color:#f92672">=</span>twitter_consumer_secret,
</span></span><span style="display:flex;"><span>        access_token<span style="color:#f92672">=</span>twitter_access_token, access_token_secret<span style="color:#f92672">=</span>twitter_access_token_secret
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Create tweet using langchain &#39;map_reduce&#39; chain output</span>
</span></span><span style="display:flex;"><span>    response <span style="color:#f92672">=</span> client<span style="color:#f92672">.</span>create_tweet(
</span></span><span style="display:flex;"><span>        text<span style="color:#f92672">=</span>output
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> func<span style="color:#f92672">.</span>HttpResponse(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;https://twitter.com/user/status/</span><span style="color:#e6db74">{</span>response<span style="color:#f92672">.</span>data[<span style="color:#e6db74">&#39;id&#39;</span>]<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><h1 id="testing-the-api"><strong>Testing the API</strong></h1>
<p>The next step after building the API is to test that it works correctly. For this purpose, I have created a new temporary Twitter account specifically for this.</p>
<p>To invoke the API, you can use any tool capable of making an HTTP request (<code>cURL</code>, <code>Postman</code>, <code>Insomnia</code>, etc.), you just have to make an HTTP call to the <code>/tweet</code> endpoint of the API.</p>
<p>Here&rsquo;s an example using <code>cURL</code> to call the API running in Azure:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>curl -X POST https://func-openai-azfunc-dev.azurewebsites.net/api/tweet?code<span style="color:#f92672">=</span>hoZ6u8lIMlhu7-Zs8gvDb04R2fXvYeapijR3YYRlgiwmAzFulsiRMA<span style="color:#f92672">==</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>     -H <span style="color:#e6db74">&#34;Content-Type: application/json&#34;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>     -d <span style="color:#e6db74">&#39;{&#34;uri&#34;: &#34;https://www.mytechramblings.com/posts/deploy-az-resources-when-not-available-on-azurerm/&#34;}&#39;</span>
</span></span></code></pre></div><p>And here&rsquo;s another example using <code>cURL</code> to call the API running locally:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>curl -X POST http://localhost:7071/api/tweet <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>     -H <span style="color:#e6db74">&#34;Content-Type: application/json&#34;</span> <span style="color:#ae81ff">\ </span> 
</span></span><span style="display:flex;"><span>     -d <span style="color:#e6db74">&#39;{&#34;uri&#34;: &#34;https://www.mytechramblings.com/posts/how-to-integrate-your-roslyn-analyzers-with-sonarqube&#34;}&#39;</span>
</span></span></code></pre></div><p>It takes around 40 to 50 seconds to create the tweet and post it to Twitter. Here&rsquo;s how the end result looks like in Twitter.</p>
<p><img alt="tweetapi-tweet-results-2" src="/img/tweetapi-tweet-results-2.png"></p>
<p>And a few more tweets:</p>
<p><img alt="tweetapi-tweet-results" src="/img/tweetapi-tweet-results.png"></p>
<p>Do you remember when in the previous section we talked about the <code>map-reduce</code> strategy making multiple calls to the LLM model? <br>
If we look at the metrics of the total calls made to Azure OpenAI to generate a single tweet, we can clearly see it.</p>
<p>The following image shows a total of 12 calls made to Azure OpenAI to generate a single tweet</p>
<p><img alt="tweetapi-openai-total-calls-metric" src="/img/tweetapi-openai-total-calls-metric.png"></p>

    </div>
    
      
        <div class="pagination">
          <div class="pagination__title">
            <span class="pagination__title-h">Read other posts</span>
            <hr />
          </div>
          <div class="pagination__buttons">
            
              <span class="button previous">
                <a href="https://www.mytechramblings.com/posts/building-qa-app-with-openai-pinecone-and-streamlit/">
                  <span class="button__icon">←</span>
                  <span class="button__text">Building a Q&amp;A app capable of answering questions related to your enterprise documents using Azure OpenAI&#39;s GPT-4, Pinecone and Streamlit.</span>
                </a>
              </span>
            
            
              <span class="button next">
                <a href="https://www.mytechramblings.com/posts/how-to-integrate-your-roslyn-analyzers-with-sonarqube/">
                  <span class="button__text">How to integrate your Roslyn Analyzer project with SonarQube</span>
                  <span class="button__icon">→</span>
                </a>
              </span>
            
          </div>
        </div>
      
    


    
      
        

      
    

    </div>

      </div>

      
        <footer class="footer">
  <div class="footer__inner">
    
      <div class="copyright">
        <span>© 2024 Powered by <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a></span>
        <span>Theme created by <a href="https://twitter.com/panr" target="_blank" rel="noopener">panr</a></span>
      </div>
    
  </div>
</footer>

<script src="https://www.mytechramblings.com/assets/main.js"></script>
<script src="https://www.mytechramblings.com/assets/prism.js"></script>
<script src='https://storage.ko-fi.com/cdn/scripts/overlay-widget.js'></script>
<script>
  kofiWidgetOverlay.draw('carlospons', {
    'type': 'floating-chat',
    'floating-chat.donateButton.text': 'Donate',
    'floating-chat.donateButton.background-color': '#ff5f5f',
    'floating-chat.donateButton.text-color': '#fff'
  });
</script>

      
    </div>

    
      
<script>
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-170300931-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
    
  </body>
</html>
